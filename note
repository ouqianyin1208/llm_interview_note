verifyer
环境： /root/anaconda3/envs/verl 
对应starfire：verifyer


代码路径：
/home/notebook/code/group/tangmaowen/verifyer
/home/notebook/code/group/tangmaowen/verl-main
数据
脚本：
/home/notebook/code/group/tangmaowen/verl-main/examples/data_preprocess/collection_list_ind.py


Point-wise数据：
/home/notebook/code/group/tangmaowen/verl-main/examples/data_preprocess/collection_data/collection_point


Point-wise+List-wise混合数据：
/home/notebook/code/group/tangmaowen/verl-main/examples/data_preprocess/collection_data/collection_list_mix_ind
 
Point-wise Prompt
def memory_match_title_old(query, chunk):
    prompt = """你是一位合集收藏软件的用户，现在你有一个自己创建的**合集名**。请你站在用户角度，根据实际生活场景，精准判断以下**收藏**是否能收录进你的合集名中。必须满足以下关键要求：


## 收录原则
1.检查**收藏**中的**内容来源**字段，若发现与**合集名**字段强关联，则一定收录，不需要查看以下规则。例如现有**合集名**为《小红书收藏》，**内容来源**为“小红书”的都需收录。
2.特殊场景收录规则
    - 穿搭场景：等同于“ootd”、“穿搭技巧”，并且当**合集名**为冬季穿搭/秋季穿搭，此时候选项中有一条内容为“秋冬毛衣搭配”，其标签或内容为“秋冬穿搭xxx”的是既指可以秋季穿也可以冬季穿，所以类似这样的**收藏**是可以被收录的。
    - **合集名**为人名类的，只要**收藏**里有相关人名均为匹配。
    - **合集名**有可能为中文拼音，英文，需要充分注意理解。
    - 美食场景：只匹配大篇幅提及美食的品尝或烹饪的**收藏**。（排除旅游攻略中次要提及的美食，排除餐厅消费订单账单）。
    - 购物场景：只匹配内容中含有具体的物品，有潜在购买倾向，出现购买动作或购买决策的痕迹，如价格、折扣、订单、购物车等的**收藏**。
    - 旅游场景：只匹配出现明确目的地，且出现旅游用途信息，如“旅游、旅行、度假、攻略、打卡、景点推荐、美食打卡”等旅游意图词的**收藏**。


3.明确排除的不能**收录**类型
    - 穿搭技巧: 不能收录""妆造""相关**收藏**。
    - 美妆推荐: 不能收录""护肤品"", ""隔离霜""相关**收藏**。


## 输出规范 ##
必须以这样的句式返回<analysis>...</analysis>,<answer>true or false</answer>。
其中<analysis>简短描述逻辑思考，尤其注意判断内容来源</analysis>,<answer>只返回true or false，true为收录，false为不收录</answer>


输入：该**合集名**是否能收录该**收藏**？
<合集名>{query}</合集名>
<收藏>{chunk}</收藏>


输出：""".format(query=query, chunk=chunk)
    return prompt


 
List-wise Prompt
def memory_match_title_listwise_ind(query, chunks):
    """多条 chunk 的 list-wise prompt"""
    chunks_text = "\n".join([f"<收藏{i+1}>{c}</收藏{i+1}>" for i, c in enumerate(chunks)])
    prompt = """你是一位合集收藏软件的用户，现在你有一个自己创建的**合集名**。请你站在用户角度，根据实际生活场景，精准判断以下多个**收藏**是否能收录进你的合集名中。必须满足以下关键要求：


## 收录原则
1.检查**收藏**中的**内容来源**字段，若发现与**合集名**字段强关联，则一定收录，不需要查看以下规则。例如现有**合集名**为《小红书收藏》，**内容来源**为“小红书”的都需收录。
2.特殊场景收录规则
    - 穿搭场景：等同于“ootd”、“穿搭技巧”，并且当**合集名**为冬季穿搭/秋季穿搭，此时候选项中有一条内容为“秋冬毛衣搭配”，其标签或内容为“秋冬穿搭xxx”的是既指可以秋季穿也可以冬季穿，所以类似这样的**收藏**是可以被收录的。
    - **合集名**为人名类的，只要**收藏**里有相关人名均为匹配。
    - **合集名**有可能为中文拼音，英文，需要充分注意理解。
    - 美食场景：只匹配大篇幅提及美食的品尝或烹饪的**收藏**。（排除旅游攻略中次要提及的美食，排除餐厅消费订单账单）。
    - 购物场景：只匹配内容中含有具体的物品，有潜在购买倾向，出现购买动作或购买决策的痕迹，如价格、折扣、订单、购物车等的**收藏**。
    - 旅游场景：只匹配出现明确目的地，且出现旅游用途信息，如“旅游、旅行、度假、攻略、打卡、景点推荐、美食打卡”等旅游意图词的**收藏**。


3.明确排除的不能**收录**类型
    - 穿搭技巧: 不能收录""妆造""相关**收藏**。
    - 美妆推荐: 不能收录""护肤品"", ""隔离霜""相关**收藏**。


## 输出规范 ##
必须以这样的句式返回<analysis>...</analysis>,<answer>[1, 3, ...]</answer>。
其中<analysis>一两句对**合集名**与**收藏X**、**收藏Y**.....的简短必要分析</analysis>,<answer>只返回需要收录的收藏编号列表。若都不收录，返回空列表[]</answer>
# 容易判断的**收藏**不进行分析但也要在<analysis>中给出明确结论，难判断的**收藏**需要联合其他**收藏**进行分析。


输入：该**合集名**能收录哪些**收藏**？
<合集名>{query}</合集名>
{chunks_text}


输出：""".format(query=query, chunks_text=chunks_text)
    return prompt


模型训练
主要使用GRPO与GSPO
训练脚本（GRPO+KL散度改进+课程学习）：
/home/notebook/code/group/tangmaowen/verl-main/examples/grpo_trainer/train_collection.sh


命令示例： bash train_collection.sh --kl 或空 --cl 或空 --reward f1/acc/pre/recall
# --kl 动态kl散度
# --cl 课程学习


GSPO训练脚本：
/home/notebook/code/group/tangmaowen/verl-main/examples/grpo_trainer/run_collection_14B_gspo.sh

set -x


python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=/mnt/workspace/tangmaowen/verl-main/examples/data_preprocess/collection_data/collection_point/train.parquet \
    data.val_files=/mnt/workspace/tangmaowen/verl-main/examples/data_preprocess/collection_data/collection_point/test.parquet \
    +data.apply_chat_template_kwargs='{enable_thinking: False}'\
    data.train_batch_size=256 \
    data.max_prompt_length=2800 \
    data.max_response_length=200 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    actor_rollout_ref.model.path=Qwen/Qwen3-14B \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.02 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.actor.policy_loss.loss_mode="gspo" \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.rollout.n=6 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.use_kl_in_reward=False \
    custom_reward_function.path=/mnt/workspace/tangmaowen/verl-main/verl/utils/reward_score/collection.py\
    custom_reward_function.name=compute_score\
    trainer.critic_warmup=0 \
    trainer.logger='["console","tensorboard"]' \
    trainer.project_name='qwen3_14b_function_gspo' \
    trainer.experiment_name='qwen3_14b_function_gspo_1030' \
    trainer.n_gpus_per_node=2 \
    trainer.nnodes=1 \
    trainer.save_freq=20 \
    trainer.test_freq=5 \
    trainer.total_epochs=20 $@

动态KL散度
损失函数路径：
/home/notebook/code/group/tangmaowen/verl-main/verl/workers/roles/utils/losses.py
 
原理：


课程学习
路径：
/home/notebook/code/group/tangmaowen/verl-main/verl/trainer/ppo/ray_trainer.py
 
采样函数：

 
EMA更新正确率权重：

 
更新逻辑：


奖励构造
奖励计算路径：
/home/notebook/code/group/tangmaowen/verl-main/verl/utils/reward_score/collection_list_ind.py
 
分为四种奖励，保F1,Acc,Precision,Recall，分别对应四个函数。




 
模型合并
/home/notebook/code/group/tangmaowen/verl-main/examples/model_merge/merge_collection.sh
 
模型评估
benchmark：
/home/notebook/code/group/tangmaowen/verifyer/LLM/data/benchmark/benchmark.xlsx


模型部署：
/home/notebook/code/group/tangmaowen/verifyer/LLM/run_model.sh


评估脚本：
/home/notebook/code/group/tangmaowen/verifyer/LLM/test_datasets.py
benchmark.xlsx内有3个表，每个表按n个样本为1个list进行一次推理。全为1则退化为Point-wise推理。




